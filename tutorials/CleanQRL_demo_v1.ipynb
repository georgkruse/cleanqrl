{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==2.1.0 in c:\\users\\kruse\\miniforge3\\envs\\cleanqrl\\lib\\site-packages (from -r requirements.txt (line 1)) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is an adaptation from https://docs.cleanrl.dev/rl-algorithms/dqn/#dqnpy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import yaml\n",
    "from ray.train._internal.session import get_session\n",
    "from replay_buffer import ReplayBuffer, ReplayBufferWrapper\n",
    "\n",
    "\n",
    "# ENV LOGIC: create your env (with config) here:\n",
    "def make_env(env_id, config):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = ReplayBufferWrapper(env)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize your agent here:\n",
    "class DQNAgentClassical(nn.Module):\n",
    "    def __init__(self, observation_size, num_actions):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(observation_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)\n",
    "\n",
    "\n",
    "def log_metrics(config, metrics, report_path=None):\n",
    "    if config[\"wandb\"]:\n",
    "        wandb.log(metrics)\n",
    "    if ray.is_initialized():\n",
    "        ray.train.report(metrics=metrics)\n",
    "    else:\n",
    "        with open(os.path.join(report_path, \"result.json\"), \"a\") as f:\n",
    "            json.dump(metrics, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# MAIN TRAINING FUNCTION\n",
    "def dqn_classical(config: dict):\n",
    "    cuda = config[\"cuda\"]\n",
    "    env_id = config[\"env_id\"]\n",
    "    num_envs = config[\"num_envs\"]\n",
    "    buffer_size = config[\"buffer_size\"]\n",
    "    total_timesteps = config[\"total_timesteps\"]\n",
    "    start_e = config[\"start_e\"]\n",
    "    end_e = config[\"end_e\"]\n",
    "    exploration_fraction = config[\"exploration_fraction\"]\n",
    "    learning_starts = config[\"learning_starts\"]\n",
    "    train_frequency = config[\"train_frequency\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    gamma = config[\"gamma\"]\n",
    "    target_network_frequency = config[\"target_network_frequency\"]\n",
    "    tau = config[\"tau\"]\n",
    "    lr = config[\"lr\"]\n",
    "\n",
    "    if config[\"seed\"] == \"None\":\n",
    "        config[\"seed\"] = None\n",
    "\n",
    "    if not ray.is_initialized():\n",
    "        report_path = config[\"path\"]\n",
    "        name = config[\"trial_name\"]\n",
    "        with open(os.path.join(report_path, \"result.json\"), \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    else:\n",
    "        session = get_session()\n",
    "        report_path = session.storage.trial_fs_path\n",
    "        name = session.storage.trial_fs_path.split(\"/\")[-1]\n",
    "\n",
    "    if config[\"wandb\"]:\n",
    "        wandb.init(\n",
    "            project=config[\"project_name\"],\n",
    "            sync_tensorboard=True,\n",
    "            config=config,\n",
    "            name=name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "            dir=report_path,\n",
    "        )\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    if config[\"seed\"] is None:\n",
    "        seed = np.random.randint(0, 1e9)\n",
    "    else:\n",
    "        seed = config[\"seed\"]\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and cuda else \"cpu\")\n",
    "    assert (\n",
    "        env_id in gym.envs.registry.keys()\n",
    "    ), f\"{env_id} is not a valid gymnasium environment\"\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(env_id, config) for i in range(num_envs)])\n",
    "    assert isinstance(\n",
    "        envs.single_action_space, gym.spaces.Discrete\n",
    "    ), \"only discrete action space is supported\"\n",
    "\n",
    "    observation_size = np.prod(envs.single_observation_space.shape)\n",
    "    num_actions = envs.single_action_space.n\n",
    "\n",
    "    # Here, the classical agent is initialized with a Neural Network\n",
    "    q_network = DQNAgentClassical(observation_size, num_actions).to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    target_network = DQNAgentClassical(observation_size, num_actions).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    rb = ReplayBuffer(\n",
    "        buffer_size,\n",
    "        envs.single_observation_space,\n",
    "        envs.single_action_space,\n",
    "        device,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    # global parameters to log\n",
    "    print_interval = 50\n",
    "    global_episodes = 0\n",
    "    episode_returns = deque(maxlen=print_interval)\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    obs, _ = envs.reset(seed=seed)\n",
    "    for global_step in range(total_timesteps):\n",
    "        # ALGO LOGIC: put action logic here\n",
    "        epsilon = linear_schedule(\n",
    "            start_e, end_e, exploration_fraction * total_timesteps, global_step\n",
    "        )\n",
    "        if random.random() < epsilon:\n",
    "            actions = np.array(\n",
    "                [envs.single_action_space.sample() for _ in range(envs.num_envs)]\n",
    "            )\n",
    "        else:\n",
    "            q_values = q_network(torch.Tensor(obs).to(device))\n",
    "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        if \"episode\" in infos:\n",
    "            for idx, finished in enumerate(infos[\"_episode\"]):\n",
    "                if finished:\n",
    "                    metrics = {}\n",
    "                    global_episodes += 1\n",
    "                    episode_returns.append(infos[\"episode\"][\"r\"].tolist()[idx])\n",
    "                    metrics[\"episode_reward\"] = infos[\"episode\"][\"r\"].tolist()[idx]\n",
    "                    metrics[\"episode_length\"] = infos[\"episode\"][\"l\"].tolist()[idx]\n",
    "                    metrics[\"global_step\"] = global_step\n",
    "                    log_metrics(config, metrics, report_path)\n",
    "\n",
    "            if global_episodes % print_interval == 0 and not ray.is_initialized():\n",
    "                print(\n",
    "                    \"Global step: \",\n",
    "                    global_step,\n",
    "                    \" Mean return: \",\n",
    "                    np.mean(episode_returns),\n",
    "                )\n",
    "\n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for idx, trunc in enumerate(truncations):\n",
    "            if trunc:\n",
    "                real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        obs = next_obs\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > learning_starts:\n",
    "            if global_step % train_frequency == 0:\n",
    "                data = rb.sample(batch_size)\n",
    "                with torch.no_grad():\n",
    "                    target_max, _ = target_network(data.next_observations).max(dim=1)\n",
    "                    td_target = data.rewards.flatten() + gamma * target_max * (\n",
    "                        1 - data.dones.flatten()\n",
    "                    )\n",
    "                old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "                loss = F.mse_loss(td_target, old_val)\n",
    "\n",
    "                if global_step % 100 == 0:\n",
    "                    metrics = {}\n",
    "                    metrics[\"td_loss\"] = loss.item()\n",
    "                    metrics[\"q_values\"] = old_val.mean().item()\n",
    "                    metrics[\"SPS\"] = int(global_step / (time.time() - start_time))\n",
    "                    log_metrics(config, metrics, report_path)\n",
    "                # optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # update target network\n",
    "            if global_step % target_network_frequency == 0:\n",
    "                for target_network_param, q_network_param in zip(\n",
    "                    target_network.parameters(), q_network.parameters()\n",
    "                ):\n",
    "                    target_network_param.data.copy_(\n",
    "                        tau * q_network_param.data\n",
    "                        + (1.0 - tau) * target_network_param.data\n",
    "                    )\n",
    "\n",
    "    if config[\"save_model\"]:\n",
    "        model_path = f\"{os.path.join(report_path, name)}.cleanqrl_model\"\n",
    "        torch.save(q_network.state_dict(), model_path)\n",
    "        print(f\"model saved to {model_path}\")\n",
    "\n",
    "    envs.close()\n",
    "    if config[\"wandb\"]:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    @dataclass\n",
    "    class Config:\n",
    "        # General parameters\n",
    "        trial_name: str = \"dqn_classical\"  # Name of the trial\n",
    "        trial_path: str = \"logs\"  # Path to save logs relative to the parent directory\n",
    "        wandb: bool = False  # Use wandb to log experiment data\n",
    "        project_name: str = \"cleanqrl\"  # If wandb is used, name of the wandb-project\n",
    "\n",
    "        # Environment parameters\n",
    "        env_id: str = \"CartPole-v1\"  # Environment ID\n",
    "\n",
    "        # Algorithm parameters\n",
    "        num_envs: int = 1  # Number of environments\n",
    "        seed: int = None  # Seed for reproducibility\n",
    "        buffer_size: int = 10000  # Size of the replay buffer\n",
    "        total_timesteps: int = 100000  # Total number of timesteps\n",
    "        start_e: float = 1.0  # Starting value of epsilon for exploration\n",
    "        end_e: float = 0.01  # Ending value of epsilon for exploration\n",
    "        exploration_fraction: float = 0.1  # Fraction of total timesteps for exploration\n",
    "        learning_starts: int = 1000  # Timesteps before learning starts\n",
    "        train_frequency: int = 1  # Frequency of training\n",
    "        batch_size: int = 32  # Batch size for training\n",
    "        gamma: float = 0.99  # Discount factor\n",
    "        target_network_frequency: int = 100  # Frequency of target network updates\n",
    "        tau: float = 0.01  # Soft update coefficient\n",
    "        lr: float = 0.01  # Learning rate for network weights\n",
    "        cuda: bool = False  # Whether to use CUDA\n",
    "        save_model: bool = True  # Save the model after the run\n",
    "\n",
    "    config = vars(Config())\n",
    "\n",
    "    # Based on the current time, create a unique name for the experiment\n",
    "    config[\"trial_name\"] = (\n",
    "        datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\") + \"_\" + config[\"trial_name\"]\n",
    "    )\n",
    "    config[\"path\"] = os.path.join(\n",
    "        Path(__file__).parent.parent, config[\"trial_path\"], config[\"trial_name\"]\n",
    "    )\n",
    "    # Create the directory and save a copy of the config file so that the experiment can be replicated\n",
    "    os.makedirs(os.path.dirname(config[\"path\"] + \"/\"), exist_ok=True)\n",
    "    config_path = os.path.join(config[\"path\"], \"config.yml\")\n",
    "    with open(config_path, \"w\") as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "    # Start the agent training\n",
    "    dqn_classical(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanqrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
